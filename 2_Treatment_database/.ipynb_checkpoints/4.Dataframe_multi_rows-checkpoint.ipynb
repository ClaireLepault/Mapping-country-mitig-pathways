{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import joblib\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4720 X 12 dataframe\n"
     ]
    }
   ],
   "source": [
    "(df) = joblib.load(\"2_Fusion/df_for_analysis.pkl\" )\n",
    "print( \"Loaded %d X %d dataframe\" % (len(df), len(df.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prepro_title'] = \" \" + df['prepro_title'] + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new dataframe containing one row for each country mentionned per paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 259 X 19 dataframe\n"
     ]
    }
   ],
   "source": [
    "UNSD_Path = \"2_Fusion/1_Extraction/UNSD_database.xlsx\"\n",
    "UNSD_df = pd.read_excel(UNSD_Path, encoding='utf-8')\n",
    "print( \"Loaded %d X %d dataframe\" % (len(UNSD_df), len(UNSD_df.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global Code</th>\n",
       "      <th>Global Name</th>\n",
       "      <th>Region Code</th>\n",
       "      <th>Region_Name</th>\n",
       "      <th>Sub-region Code</th>\n",
       "      <th>Sub-region_Name</th>\n",
       "      <th>Intermediate Region Code</th>\n",
       "      <th>Intermediate_Region_Name</th>\n",
       "      <th>Country0</th>\n",
       "      <th>Demonym1</th>\n",
       "      <th>Demonym2</th>\n",
       "      <th>M49 Code</th>\n",
       "      <th>ISO_3</th>\n",
       "      <th>Least Developed Countries (LDC)</th>\n",
       "      <th>Land Locked Developing Countries (LLDC)</th>\n",
       "      <th>Small Island Developing States (SIDS)</th>\n",
       "      <th>Developed_Developing_Countries</th>\n",
       "      <th>Region</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>World</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Africa</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Algerian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>DZA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Developing</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Global Code Global Name  Region Code Region_Name  Sub-region Code  \\\n",
       "0          1.0       World          2.0      Africa             15.0   \n",
       "\n",
       "   Sub-region_Name  Intermediate Region Code Intermediate_Region_Name  \\\n",
       "0  Northern Africa                       NaN                      NaN   \n",
       "\n",
       "  Country0  Demonym1 Demonym2 M49 Code ISO_3 Least Developed Countries (LDC)  \\\n",
       "0  Algeria  Algerian      NaN       12   DZA                             NaN   \n",
       "\n",
       "  Land Locked Developing Countries (LLDC)  \\\n",
       "0                                     NaN   \n",
       "\n",
       "  Small Island Developing States (SIDS) Developed_Developing_Countries  \\\n",
       "0                                   NaN                     Developing   \n",
       "\n",
       "   Region  Country  \n",
       "0  Africa  Algeria  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNSD_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe where we concatenate all the sub-dataframes corresponding to each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = pd.DataFrame()\n",
    "table_remove_punct_white_space = str.maketrans(string.punctuation, ' '*len(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the 'Country0' column to preprocess punctuation as in the title (e.g. for Côte d'Ivoire) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4928 X 13 dataframe with one line for each country search\n"
     ]
    }
   ],
   "source": [
    "for index, row in UNSD_df.iterrows():\n",
    "    country = str(row['Country0'])\n",
    "    country_low = country.lower().translate(table_remove_punct_white_space)\n",
    "    country_low = ' '.join(country_low.split())\n",
    "    country_low = ' ' + country_low + ' '\n",
    "    demon1 = str(row['Demonym1'])\n",
    "    demon1_low = ' ' + demon1.lower() + ' '\n",
    "    demon2 = str(row['Demonym2'])\n",
    "    demon2_low = ' ' + demon2.lower() + ' '\n",
    "    if (demon1 == \"nan\") & (demon2 == \"nan\"):\n",
    "        df_tempo = df[df['prepro_title'].str.contains(country_low)].copy()\n",
    "        df_tempo['Country0'] = country\n",
    "        df_country = df_country.append(df_tempo)\n",
    "    elif (demon1 != \"nan\") & (demon2 == \"nan\"):\n",
    "        df_tempo = df[df['prepro_title'].str.contains(country_low) | df['prepro_title'].str.contains(demon1_low)].copy()\n",
    "        df_tempo['Country0'] = country\n",
    "        df_country = df_country.append(df_tempo)\n",
    "    else:\n",
    "        df_tempo = df[df['prepro_title'].str.contains(country_low) | df['prepro_title'].str.contains(demon1_low)| df['prepro_title'].str.contains(demon2_low)].copy()\n",
    "        df_tempo['Country0'] = country\n",
    "        df_country = df_country.append(df_tempo)        \n",
    "print(\"%d X %d dataframe with one line for each country search\" % (len(df_country), len(df_country.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create region & ISO-3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = pd.merge(df_country,UNSD_df[['Country0','ISO_3','Region']],on='Country0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates according ISO-3 code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4914 X 15 dataframe\n"
     ]
    }
   ],
   "source": [
    "df_country = df_country.drop_duplicates(['ISO_3','title'], keep='first')\n",
    "print(\"%d X %d dataframe\" % (len(df_country), len(df_country.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create horizon year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verif = df_country.copy()\n",
    "df_verif.reset_index(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_verif['horizon_year'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_liste = []\n",
    "for k in range (2025, 2101):\n",
    "    year_liste.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in year_liste:\n",
    "    year = str(year)\n",
    "    presence_title = df_verif.title.str.contains(year, regex = False, na = False)\n",
    "    presence_abstract = df_verif.abstract.str.contains(year, regex = False, na = False)\n",
    "    presence_authorkeywords = df_verif.author_keywords.str.contains(year, regex = False, na = False)\n",
    "    for k in range(len(df_verif)):\n",
    "        if ((presence_title[k]== True) or (presence_abstract[k]== True) or (presence_authorkeywords[k]== True)) :\n",
    "            df_verif.loc[k,'horizon_year'] = year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only rows that contains a horizon year in [2025;2100] in title, abstract or AUTHOR keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_horizon_year = df_verif[df_verif.horizon_year.isnull()]\n",
    "df_no_horizon_year.to_excel('no_horizon_year.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908 X 16 dataframe with horizon_year\n"
     ]
    }
   ],
   "source": [
    "df_verif = df_verif[df_verif.horizon_year.notnull()]\n",
    "print( \"%d X %d dataframe with horizon_year\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Associate to ISO3 one only name (USA, United States, US, U.S. -> Country = United States of America)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNSD_unique = UNSD_df.groupby('ISO_3').first().reset_index()\n",
    "df_verif = pd.merge(df_verif, UNSD_unique[['ISO_3','Country']],on='ISO_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for US papers : ambiguity between country and personal pronoun 'us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = df_verif[(df_verif['prepro_title'].str.contains(' us ')) & \n",
    "                 (~df_verif['prepro_title'].str.contains(' the us ')) & \n",
    "                 (~df_verif['title'].str.contains('US')) & \n",
    "                 (df_verif['Country0']==\"US\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962     Hybrids are an effective transitional technology for limiting us passenger fleet carbon emissions                                                                                   \n",
      "970     What do global climate models tell us about future arctic sea ice coverage changes?                                                                                                 \n",
      "975     Current fossil fuel infrastructure does not yet commit us to 1.5 °C warming                                                                                                         \n",
      "988     Pursuing necessary reductions in embedded GHG emissions of developed nations: Will efficiency improvements and changes in consumption get us there?                                 \n",
      "1018    Limiting global warming to 2 °C: What do the latest mitigation studies tell us about costs, technologies and other impacts?                                                         \n",
      "1030    What are incident reports telling us? A comparative study at two Australian hospitals of medication errors identified at audit, detected by staff and reported to an incident system\n",
      "1035    How negative can biofuels with CCS take us and at what cost? Refining the economic potential of biofuel production with CCS using spatially-explicit modeling                       \n",
      "1038    What do greenhouse gas scenarios tell us?                                                                                                                                           \n",
      "1057    A comparative assessment of electric propulsion systems in the 2030 us light-duty vehicle fleet                                                                                     \n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'max_colwidth', -1):\n",
    "    print (check['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4901 X 17 dataframe after verification\n"
     ]
    }
   ],
   "source": [
    "df_verif = df_verif.drop([970,975,988,1018,1030,1035,1038])\n",
    "print(\"%d X %d dataframe after verification\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for US papers : ambiguity between country and dollars 'US$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = df_verif[(df_verif['title'].str.contains(\"\\$\")) & \n",
    "                 (df_verif['Country']==\"USA\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986    Is LNG really a 'US$22 billion distraction'?\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'max_colwidth', -1):\n",
    "    print (check['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900 X 17 dataframe after verification\n"
     ]
    }
   ],
   "source": [
    "df_verif = df_verif.drop([986])\n",
    "print(\"%d X %d dataframe after verification\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the attribution of papers containing the terms \"British Colombia\" to the UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4887 X 17 dataframe after verification\n"
     ]
    }
   ],
   "source": [
    "df_verif = df_verif.drop(df_verif[(df_verif['ISO_3'].str.contains(\"GBR\")) & (df_verif.prepro_title.str.contains(\"british columbia\"))].index)\n",
    "print(\"%d X %d dataframe after verification\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the attribution of papers containing the terms \"New Jersey\" to Jersey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4884 X 17 dataframe after verification\n"
     ]
    }
   ],
   "source": [
    "df_verif = df_verif.drop(df_verif[(df_verif['ISO_3'].str.contains(\"JEY\")) & (df_verif.prepro_title.str.contains(\"new jersey\"))].index)\n",
    "print(\"%d X %d dataframe after verification\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4884 X 17 dataframe with one line for each country\n"
     ]
    }
   ],
   "source": [
    "df_multi = df_verif.copy()\n",
    "print(\"%d X %d dataframe with one line for each country\" % (len(df_multi), len(df_multi.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4884 X 14 multi-rows dataframe\n"
     ]
    }
   ],
   "source": [
    "col = ['ISO_3','Country','Region','title','authors','source','doi','doc_type','abstract','author_keywords','publication_year','horizon_year','scopus_number','WOS_number']\n",
    "df_multi = df_multi.reindex(columns=col)\n",
    "df_multi.sort_values(by = ['ISO_3','Country','publication_year','title','doi'], ascending = [True,True,False,True,True], inplace = True)\n",
    "print( \"Loaded %d X %d multi-rows dataframe\" % (len(df_multi), len(df_multi.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi.to_excel('database_multi_rows_each_paper.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database one row each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = df_multi.groupby(['title']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4691 X 14 one row each paper dataframe\n"
     ]
    }
   ],
   "source": [
    "col = ['ISO_3','Country','Region','title','authors','source','doi','doc_type','abstract','author_keywords','publication_year','horizon_year','scopus_number','WOS_number']\n",
    "df_one = df_one.reindex(columns=col)\n",
    "df_one.sort_values(by = ['ISO_3','Country','publication_year','title','doi'], ascending = [True,True,False,True,True], inplace = True)\n",
    "print( \"Loaded %d X %d one row each paper dataframe\" % (len(df_one), len(df_one.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.reset_index(0,inplace = True)\n",
    "df_one.drop('index',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.to_excel('database_one_row_each_paper.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store dataframe multi papers and dataframe unique paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2df_countries_title.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((df_multi, df_one), \"2df_countries_title.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count papers from Scopus and WOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944 papers only on Scopus\n",
      "574 papers only on WOS\n",
      "3173 papers on both Scopus and WOS\n"
     ]
    }
   ],
   "source": [
    "df_only_scop = df_one[df_one.scopus_number.notnull() & df_one.WOS_number.isnull()]\n",
    "print(\"%d papers only on Scopus\" % len(df_only_scop) )\n",
    "\n",
    "df_only_WOS = df_one[df_one.scopus_number.isnull() & df_one.WOS_number.notnull()]\n",
    "print(\"%d papers only on WOS\" % len(df_only_WOS) )\n",
    "\n",
    "df_both = df_one[df_one.scopus_number.notnull() & df_one.WOS_number.notnull()]\n",
    "print(\"%d papers on both Scopus and WOS\" % len(df_both) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store dataframe with papers mentionning no country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 rejected papers\n"
     ]
    }
   ],
   "source": [
    "df_all = df.merge(df_one, on=['title'], how='left', indicator=True)\n",
    "df_rejected = df_all[~df_all._merge.isin(['both'])]\n",
    "print(\"%d rejected papers\" % len(df_rejected) )\n",
    "df_rejected.to_excel('rejected.xlsx')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
