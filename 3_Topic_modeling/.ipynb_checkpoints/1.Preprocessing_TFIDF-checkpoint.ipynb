{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import joblib\n",
    "import string\n",
    "from stemming.porter2 import stem\n",
    "from nltk.corpus import stopwords\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the database with one row for each publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4691 X 16 dataframe with unique papers\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../2_Treatment_database/output/database_one_row_each_paper.csv\")\n",
    "print( \"Loaded %d X %d dataframe with unique papers\" % (len(df), len(df.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform abstracts in lower cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = df['abstract'].str.lower().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation and replace it by on white space and then replace double white by one white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation =  string.punctuation + '’—☆–−()()©“”‘'\n",
    "table_remove_punct_white_space = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "abstracts = [abstract.translate(table_remove_punct_white_space) for abstract in abstracts]\n",
    "abstracts = [' '.join(abstract.split()) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define stopwords \n",
    "* nltk stopwords\n",
    "* countries and demonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "countriesPath = \"../0_Reference_files/UNSD_database.csv\"\n",
    "countries_df = pd.read_csv(countriesPath)\n",
    "countries_list = countries_df['Country0'].tolist()\n",
    "demonym1_list = countries_df['Demonym1'][countries_df['Demonym1'].notnull()].tolist()\n",
    "demonym2_list = countries_df['Demonym2'][countries_df['Demonym2'].notnull()].tolist()\n",
    "names_list = countries_list + demonym1_list + demonym2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = [str(name).lower().translate(table_remove_punct_white_space) for name in names_list]\n",
    "names_list = [' '.join(name.split()) for name in names_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove names that are not single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_single = [name for name in names_list\n",
    "                     if (len(name)>3 & len(name.split())>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [''] *len(not_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = {name:delet for name,delet in zip(not_single,l)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in trans.items():\n",
    "    for i in range(len(abstracts)):\n",
    "        abstracts[i] = abstracts[i].replace(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = nltk_stopwords + names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords and group words by stem :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [\" \".join([stem(word) for word in abstract.split(\" \") \n",
    "                       if ((word not in custom_stopwords) & (len(word)>2))]) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stem stopwords \n",
    "* synonyms of mitigation\n",
    "* that appeared useless after topic modeling experiences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_stopwords = ['mitig','carbon','decarbonis','decarbon','co2','ghg','greenhous','gas','emiss','reduc','reduct']\n",
    "stem_stopwords += ['million','ton','billion','proceed','paper','data','present','result','studi','describ','also','base','explor','analys','analyz','elsevi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [\" \".join([stem for stem in abstract.split(\" \") if stem not in stem_stopwords]) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts =[\" \".join([stem for stem in abstract.split(\" \") if not stem.isdigit()]) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save processed abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_prepro = pd.DataFrame(abstracts, columns = ['abstracts_prepro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_prepro.to_csv(\"./interm/processed_abstracts.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term weighting with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4691 X 1300 TF-IDF-normalized document-term matrix\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=0.01,smooth_idf= False)\n",
    "                                   \n",
    "tfidf = tfidf_vectorizer.fit_transform(abstracts)\n",
    "print('Created %d X %d TF-IDF-normalized document-term matrix' % (tfidf.shape[0], tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 1300 distinct terms\n"
     ]
    }
   ],
   "source": [
    "# extract the resulting vocabulary\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(tfidf_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save TF-IDF matrix and vocabulary features names under pkl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./interm/tfidf_matrix-features_names.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((tfidf, tfidf_feature_names), \"./interm/tfidf_matrix-features_names.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at weightest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_terms(tfidf, terms):\n",
    "    # get the sums over each column\n",
    "    sums = tfidf.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key = operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. energi (398.26)\n",
      "02. scenario (216.68)\n",
      "03. climat (196.51)\n",
      "04. chang (177.69)\n",
      "05. electr (174.56)\n",
      "06. model (174.07)\n",
      "07. use (172.87)\n",
      "08. polici (168.62)\n",
      "09. power (168.03)\n",
      "10. sector (161.10)\n",
      "11. develop (153.57)\n",
      "12. technolog (146.68)\n",
      "13. increas (145.72)\n",
      "14. cost (143.19)\n",
      "15. system (133.95)\n",
      "16. target (127.75)\n",
      "17. industri (124.57)\n",
      "18. renew (123.35)\n",
      "19. futur (115.95)\n",
      "20. generat (115.86)\n"
     ]
    }
   ],
   "source": [
    "ranking = rank_terms(tfidf,tfidf_feature_names)\n",
    "for i, pair in enumerate(ranking[0:20]):\n",
    "    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
