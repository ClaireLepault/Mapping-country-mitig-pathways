{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import joblib\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4714 X 12 dataframe\n"
     ]
    }
   ],
   "source": [
    "(df) = joblib.load(\"2_Fusion/df_for_analysis.pkl\" )\n",
    "print( \"Loaded %d X %d dataframe\" % (len(df), len(df.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prepro_title'] = \" \" + df['prepro_title'] + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new dataframe containing one row for each country mentionned per paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 262 X 19 dataframe\n"
     ]
    }
   ],
   "source": [
    "UNSD_Path = \"2_Fusion/1_Extraction/UNSD_database.xlsx\"\n",
    "UNSD_df = pd.read_excel(UNSD_Path, encoding='utf-8')\n",
    "print( \"Loaded %d X %d dataframe\" % (len(UNSD_df), len(UNSD_df.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global Code</th>\n",
       "      <th>Global Name</th>\n",
       "      <th>Region Code</th>\n",
       "      <th>Region_Name</th>\n",
       "      <th>Sub-region Code</th>\n",
       "      <th>Sub-region_Name</th>\n",
       "      <th>Intermediate Region Code</th>\n",
       "      <th>Intermediate_Region_Name</th>\n",
       "      <th>Country0</th>\n",
       "      <th>Demonym1</th>\n",
       "      <th>Demonym2</th>\n",
       "      <th>M49 Code</th>\n",
       "      <th>ISO_3</th>\n",
       "      <th>Least Developed Countries (LDC)</th>\n",
       "      <th>Land Locked Developing Countries (LLDC)</th>\n",
       "      <th>Small Island Developing States (SIDS)</th>\n",
       "      <th>Developed_Developing_Countries</th>\n",
       "      <th>Region</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>World</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Africa</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Northern Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Algerian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>DZA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Developing</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Global Code Global Name  Region Code Region_Name  Sub-region Code  \\\n",
       "0          1.0       World          2.0      Africa             15.0   \n",
       "\n",
       "   Sub-region_Name  Intermediate Region Code Intermediate_Region_Name  \\\n",
       "0  Northern Africa                       NaN                      NaN   \n",
       "\n",
       "  Country0  Demonym1 Demonym2 M49 Code ISO_3 Least Developed Countries (LDC)  \\\n",
       "0  Algeria  Algerian      NaN       12   DZA                             NaN   \n",
       "\n",
       "  Land Locked Developing Countries (LLDC)  \\\n",
       "0                                     NaN   \n",
       "\n",
       "  Small Island Developing States (SIDS) Developed_Developing_Countries  \\\n",
       "0                                   NaN                     Developing   \n",
       "\n",
       "   Region  Country  \n",
       "0  Africa  Algeria  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNSD_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe where we concatenate all the sub-dataframes corresponding to each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4923 X 13 dataframe with one line for each country search\n"
     ]
    }
   ],
   "source": [
    "for index, row in UNSD_df.iterrows():\n",
    "    country = str(row['Country0'])\n",
    "    country_low = ' ' + country.lower() + ' '\n",
    "    demon1 = str(row['Demonym1'])\n",
    "    demon1_low = ' ' + demon1.lower() + ' '\n",
    "    demon2 = str(row['Demonym2'])\n",
    "    demon2_low = ' ' + demon2.lower() + ' '\n",
    "    if (demon1 == \"nan\") & (demon2 == \"nan\"):\n",
    "        df_tempo = df[df['prepro_title'].str.contains(country_low)].copy()\n",
    "        df_tempo['Country0'] = country\n",
    "        df_country = df_country.append(df_tempo)\n",
    "    elif (demon1 != \"nan\") & (demon2 == \"nan\"):\n",
    "        df_tempo = df[df['prepro_title'].str.contains(country_low) | df['prepro_title'].str.contains(demon1_low)].copy()\n",
    "        df_tempo['Country0'] = country\n",
    "        df_country = df_country.append(df_tempo)\n",
    "    else:\n",
    "        df_tempo = df[df['prepro_title'].str.contains(country_low) | df['prepro_title'].str.contains(demon1_low)| df['prepro_title'].str.contains(demon2_low)].copy()\n",
    "        df_tempo['Country0'] = country\n",
    "        df_country = df_country.append(df_tempo)        \n",
    "print(\"%d X %d dataframe with one line for each country search\" % (len(df_country), len(df_country.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create region & ISO-3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = pd.merge(df_country,UNSD_df[['Country0','ISO_3','Region']],on='Country0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates according ISO-3 code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4905 X 15 dataframe\n"
     ]
    }
   ],
   "source": [
    "df_country = df_country.drop_duplicates(['ISO_3','title'], keep='first')\n",
    "print(\"%d X %d dataframe\" % (len(df_country), len(df_country.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only rows that contains mitigation synonym in title, abstract or AUTHOR keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country['low_abstract'] = df_country['abstract'].str.lower()\n",
    "df_country['low_author_keywords'] = df_country['author_keywords'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitig_list = [\"mitigation\",\"carbon\",\"co2\", \"ghg\",\"greenhouse gas\",\"emission\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verif = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12528 X 17 dataframe with mitigation synonym inside each row\n"
     ]
    }
   ],
   "source": [
    "for mitig in mitig_list:\n",
    "    mitig = str(mitig)\n",
    "    df_tempo = df_country[df_country['prepro_title'].str.contains(mitig) | df_country['low_abstract'].str.contains(mitig) | df_country['low_author_keywords'].str.contains(mitig, na = False)].copy()\n",
    "    df_verif = df_verif.append(df_tempo)\n",
    "print(\"%d X %d dataframe with mitigation synonym inside each row\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4721 X 17 verified dataframe\n"
     ]
    }
   ],
   "source": [
    "df_verif = df_verif.drop_duplicates(['ISO_3','title'], keep='first')\n",
    "print(\"%d X %d verified dataframe\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating horizon year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_verif.reset_index(0, inplace = True)\n",
    "df_verif['horizon_year'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_liste = []\n",
    "for k in range (2025, 2101):\n",
    "    year_liste.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in year_liste:\n",
    "    year = str(year)\n",
    "    presence_title = df_verif.title.str.contains(year, regex = False, na = False)\n",
    "    presence_abstract = df_verif.abstract.str.contains(year, regex = False, na = False)\n",
    "    presence_authorkeywords = df_verif.author_keywords.str.contains(year, regex = False, na = False)\n",
    "    for k in range(len(df_verif)):\n",
    "        if ((presence_title[k]== True) or (presence_abstract[k]== True) or (presence_authorkeywords[k]== True)) :\n",
    "            df_verif.loc[k,'horizon_year'] = year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only rows that contains a horizon year in [2025;2100] in title, abstract or AUTHOR keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4715 X 18 dataframe with horizon_year\n"
     ]
    }
   ],
   "source": [
    "df_verif = df_verif[df_verif.horizon_year.notnull()]\n",
    "print( \"%d X %d dataframe with horizon_year\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Associate to ISO3 one only name (USA, United States, US, U.S. -> Country = United States of America)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNSD_unique = UNSD_df.groupby('ISO_3').first().reset_index()\n",
    "df_verif = pd.merge(df_verif, UNSD_unique[['ISO_3','Country']],on='ISO_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for US papers : ambiguity between country and personal pronoun 'us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = df_verif[(df_verif['prepro_title'].str.contains(' us ')) & \n",
    "                 (~df_verif['prepro_title'].str.contains(' the us ')) & \n",
    "                 (~df_verif['title'].str.contains('US')) & \n",
    "                 (df_verif['Country0']==\"US\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671    What do global climate models tell us about future arctic sea ice coverage changes?                                                                                                 \n",
      "675    Current fossil fuel infrastructure does not yet commit us to 1.5 °C warming                                                                                                         \n",
      "696    Limiting global warming to 2 °C: What do the latest mitigation studies tell us about costs, technologies and other impacts?                                                         \n",
      "703    What are incident reports telling us? A comparative study at two Australian hospitals of medication errors identified at audit, detected by staff and reported to an incident system\n",
      "813    Hybrids are an effective transitional technology for limiting us passenger fleet carbon emissions                                                                                   \n",
      "832    How negative can biofuels with CCS take us and at what cost? Refining the economic potential of biofuel production with CCS using spatially-explicit modeling                       \n",
      "841    A comparative assessment of electric propulsion systems in the 2030 us light-duty vehicle fleet                                                                                     \n",
      "949    Pursuing necessary reductions in embedded GHG emissions of developed nations: Will efficiency improvements and changes in consumption get us there?                                 \n",
      "952    What do greenhouse gas scenarios tell us?                                                                                                                                           \n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'max_colwidth', -1):\n",
    "    print (check['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4709 X 19 dataframe after selection\n"
     ]
    }
   ],
   "source": [
    "df_verif = df_verif.drop([571,592,599,713,827,830])\n",
    "print(\"%d X %d dataframe after selection\" % (len(df_verif), len(df_verif.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the attribution of papers containing the term \"British Colombia\" to the UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verif = df_verif.drop(df_verif[(df_verif['ISO_3'].str.contains(\"GBR\")) & (df_verif.title.str.contains(\"British Columbia\"))].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4696 X 19 dataframe with one line for each country\n"
     ]
    }
   ],
   "source": [
    "df_multi = df_verif.copy()\n",
    "print(\"%d X %d dataframe with one line for each country\" % (len(df_multi), len(df_multi.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4696 X 14 multi-rows dataframe\n"
     ]
    }
   ],
   "source": [
    "col = ['ISO_3','Country','Region','title','authors','source','doi','doc_type','abstract','author_keywords','publication_year','horizon_year','scopus_number','WOS_number']\n",
    "df_multi = df_multi.reindex(columns=col)\n",
    "df_multi.sort_values(by = ['ISO_3','Country','publication_year','title','doi'], ascending = [True,True,False,True,True], inplace = True)\n",
    "print( \"Loaded %d X %d multi-rows dataframe\" % (len(df_multi), len(df_multi.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi.to_excel('database_multi_rows_each_paper_low_str.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database one row each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = df_multi.groupby(['title']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4516 X 14 one row each paper dataframe\n"
     ]
    }
   ],
   "source": [
    "col = ['ISO_3','Country','Region','title','authors','source','doi','doc_type','abstract','author_keywords','publication_year','horizon_year','scopus_number','WOS_number']\n",
    "df_one = df_one.reindex(columns=col)\n",
    "df_one.sort_values(by = ['ISO_3','Country','publication_year','title','doi'], ascending = [True,True,False,True,True], inplace = True)\n",
    "print( \"Loaded %d X %d one row each paper dataframe\" % (len(df_one), len(df_one.columns) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.to_excel('database_one_row_each_paper_low_str.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count papers with DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3950 X 14 dataframe with doi\n"
     ]
    }
   ],
   "source": [
    "df_doi = df_one.loc[df_one.doi.notnull()]\n",
    "print( \"Loaded %d X %d dataframe with doi\" % (len(df_doi), len(df_doi.columns) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count papers with word \"scenario\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2306 publications with 'scenario' \n"
     ]
    }
   ],
   "source": [
    "df_scenario = df_one[df_one['title'].str.contains(\"scenario\") | df_one['abstract'].str.contains(\"scenario\")| df_one['author_keywords'].str.contains(\"scenario\")].copy()\n",
    "print( \"Loaded %d publications with 'scenario' \" % (len(df_scenario)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count papers without author keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n"
     ]
    }
   ],
   "source": [
    "df_keys_nul = df_one[df_one.author_keywords.isnull()]\n",
    "print(len(df_keys_nul))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing dataframe multi papers and dataframe unique paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2df_countries_title.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((df_multi, df_one), \"2df_countries_title.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting papers from Scopus and WOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925 papers only on Scopus\n",
      "452 papers only on WOS\n",
      "3139 papers on both Scopus and WOS\n"
     ]
    }
   ],
   "source": [
    "df_only_scop = df_one[df_one.scopus_number.notnull() & df_one.WOS_number.isnull()]\n",
    "print(\"%d papers only on Scopus\" % len(df_only_scop) )\n",
    "\n",
    "df_only_WOS = df_one[df_one.scopus_number.isnull() & df_one.WOS_number.notnull()]\n",
    "print(\"%d papers only on WOS\" % len(df_only_WOS) )\n",
    "\n",
    "df_both = df_one[df_one.scopus_number.notnull() & df_one.WOS_number.notnull()]\n",
    "print(\"%d papers on both Scopus and WOS\" % len(df_both) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing dataframe with papers mentionning no country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df.merge(df_one, on=['title'], how='left', indicator=True)\n",
    "df_test = df_all[~df_all._merge.isin(['both'])]\n",
    "df_test.to_excel('rejected.xlsx')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
